# backend/utils/rag_module.py

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage
from dotenv import load_dotenv
import os

load_dotenv()

# Chroma DB ê²½ë¡œ ì„¤ì •
CHROMA_DB_DIR = "./output/chroma_db"

# ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
vector_db = Chroma(
    persist_directory=CHROMA_DB_DIR,
    embedding_function=embedding_model
)

# LLM ì²´ì¸ êµ¬ì„±
llm = ChatUpstage()

prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
        ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ ì£¼ì–´ì— ì§‘ì¤‘í•´ì„œ ì£¼ì–´ì§„ CONTEXT(ë¬¸ë§¥) ì•ˆì—ì„œë§Œ ë‹µë³€í•˜ëŠ” ì •ì§í•˜ê³  ì •í™•í•œ ë‹µë³€ ë¹„ì„œìž…ë‹ˆë‹¤.
        ì•„ëž˜ CONTEXT ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        ë§Œì•½ CONTEXTì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ëª¨ë¥´ëŠ” ë¶€ë¶„ì€ ì†”ì§í•˜ê²Œ "ì£¼ì–´ì§„ ë¬¸ë§¥ë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
        ì¶”ê°€ì ì¸ ìƒìƒì´ë‚˜ ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

        ê°€ëŠ¥í•˜ë©´ ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìž‘ì„±í•˜ì„¸ìš”. ë‚´ìš©ì´ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë‹µë³€í•˜ì„¸ìš”.
        ---
        CONTEXT:
        {context}
        """
    ),
    ("human", "{input}"),
])

rag_chain = prompt | llm | StrOutputParser()

# ðŸ” í•µì‹¬ í•¨ìˆ˜
def get_recycling_answer(question: str) -> str:
    retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={"k": 3})
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])

    return rag_chain.invoke({
        "context": context,
        "input": question
    })
